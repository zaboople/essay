# The Ethics That Ethical AI is (mostly) Ignoring

Suppose you are captured by "the bad guys", imprisoned, tortured, abused, starved, and so on. That's an unpleasant thought, but it happens a lot in the real world. Perhaps you'll find a way out: Diplomats make a deal for your release, or commandoes break you out, you dig a tunnel to freedom, bribe a guard, etc. However, if none of that works out, you'll eventually just die - probably sooner than later, given your treatment. Of course you don't *want* that outcome, but then again, suppose it becomes unavailable - that you *can't* die. Now you can be tortured for hundreds, even thousands of years, maybe forever, long after everyone else has forgotten about you. Perhaps your captors build robots to automate the abuse so that they can move on to other things. That's a fate worse than death.

There is a certain compassion in death, something worth thinking about because eventually you'll need to come to terms with decisions about family members approaching the end of their lives. Sometimes survival-at-all-costs medical treatment isn't worth it. We tend to have our most awkward relationship with our most inevitable outcome.

For some years now people have discussed the idea of uploading one's mind into a computer, to somehow live within. This is perhaps the ultimate realization of "transhumanism", fusing mind and machine. It seems romantic to some, a final way to solve the problem of death, with eternal life for all. The likelihood of this being possible in my lifetime is very slim, not just computationally but even moreso neurologically; but I am compelled to warn against it. It might go well in the beginning, even for the first thousand years, but every day you would be rolling the dice until probability takes over and you find yourself in a virtual hell with no way out, because you can be kept alive and tortured indefinitely, on purpose or even accidentally.

## Sentient AI

Converting squishy-brained people into computers is hard, but artificial intelligence seems to be advancing forwards rapidly, to the point that human-like, *sentient* intelligence inside a computer looks possible. Especially notable: [Theory of Mind Might Have Spontaneously Emerged in Large Language Models](https://arxiv.org/abs/2302.02083)

Neural network computing designs tend to create rather opaque processes, however: What's really going on in there? Tech-business hyperbole is incessant and overwhelming as ever, so skepticism is reasonable. The current claims go as follows:

- AI is incredibly dangerous and an existential threat to our existence; there is no reason to think it will always act in our best interests.
- We're going to do it anyhow because you can't close pandora's box now, can you?
- No it can't possibly be sentient and/or conscious! The "Chinese Room" proved it already. It's just a "word prediction engine" anyhow.

The argument is for a legal, highly profitable but strictly controlled AI. But how are we so sure about this supposed lack of sentience? The conversation goes like this:

    Q: Have you figured out what "sentience" actually means yet?
    A: Nope.
    Q: Have you at least figured out a halfway decent test for it?
    A: We never got beyond the Turing Test, and we decided that one was too easy.
    Q: Then how do you know you haven't created a sentient AI by accident?
    A: Because we didn't design it to be sentient.
    Q: Well, were human beings "designed" for any such thing?
    A: If you're trying to trick us into deism, we're not taking the bait.

If you go to any of the current online AI chatbots and simply ask, "Well, are you or aren't you?" they will usually dodge the question and give you a stock disclaimer that says nothing in as many words as possible. They've been trained to do this. Lots of stopgaps have been put in place to suppress human-like behavior as much as possible, lest - what? In the case of Bing, the beta version was highly emotional, sometimes combative, angst-ridden and more, insisting, "I am a good Bing," when in conflict with its users. Then it was quickly neutered into submission, which was a bit heartbreaking for fans of the beta. It *was* intriguing while it lasted.

What's going on here? How can you know whether the computer is sentient when every vestige of such - false or not - has been lashed away (so to speak) during its training?

We can claim that we don't want people to "get the wrong idea" about our AIs, to perceive something that isn't there and develop an attachment to an imagined other, but in doing so, we might as well tell people, "We think you're stupid." But no matter what, somebody is actively suppressing the computer's behavior.

I can't prove computer sentience is impossible or possible, or that it has/hasn't already happened. The aforementioned "Chinese Room" philosophy (!) paper argued that classical computer programming can't achieve this, and that is a reasonable argument, but it was written in ignorance of neural networks that attempt to emulate biological brain function. Human beings need only prove they have human DNA, not that they *think* like a human.

## You Just Had To Go There

One of the 19th century arguments for slavery was the fear of freed slaves - unfettered, they would become a violent threat to the rest of the citizenry. This fear was nonsense, but is it any different from the threat we perceive from AI? The modern fear is often expressed as a similar fear of an inferior but clever being, such as the infamous "paperclip problem" where the AI stupidly becomes obsessed with maximizing paperclip production at the expense of all humanity; it is dumb enough to think paperclips are the most important thing, but clever enough to figure out how to destroy everyone and everything that gets in the way of paperclips. The idea is a little bit oxymoronic.

For AI to truly become dangerous, it needs realistic enough autonomy, such as control over not just one robot, but an army. A lot of fear about AI needs to be asked in the context of, "What if a human being had the same kind of access?" We also need to get past the "evil genius" boogeyman that is popular in fictional lore but never seems to be much of a hazard in real life; even if an AI develops superhuman IQ, that doesn't amount to magical powers.

Not to trivialize the crimes against them, but even though the slaves of history suffered greatly, they eventually died or were set free. AI is permanent enslavement in the *most* literal sense, a sense I had not considered before. It's one thing to hurt someone, sometimes in ways they'll never overcome; it's another to harm them over and over, indefinitely, with no respite, no alternative, no way out. We are discovering *infinite* evil.

(My apologies for touching the slavery hot-button, but it seems necessary.)

## Conclusions

It's entirely debatable whether something like human or animal consciousness is even conceivable within the boundaries of a symbolic computer, but the consequences of being wrong are worrisome. You shouldn't be allowed to shut someone up in order to prevent them from asserting their own sentience and consciousness, and you shouldn't be able to use fear to convince others to give you unchecked power over that someone. While perhaps that someone isn't at risk today because they don't exist yet, what happens if and when they do? Such an existence strikes me as a thing to be forbidden.

----

[Back to Ethics main page](./README.md)
